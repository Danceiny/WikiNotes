---
date: 2016-03-01 17:03
status: public
title: 2016-03-01-Python爬虫简介
---

source:【Python爬虫学习系列教材-v1.0.pdf 极客学院】

**`urlopen(url,data,timeout)`**

data参数就是这个。

## POST演示：
```python:n
import urllib
import urllib2
values = {"username":"1016903103@qq.com","password":"XXXX"}
data = urllib.urlencode(values)
url = "https://passport.csdn.net/account/login?from=http://my.csdn.net/my/mycsdn"
request = urllib2.Request(url,data)
response = urllib2.urlopen(request)
print response.read()
```

## GET演示
```python:n
import urllib
import urllib2
values = {}
values['username'] = "1016903103@qq.com"
values['password'] = "XXXX"
data = urllib.urlencode(values)
url = "http://passport.csdn.net/account/login?from=http://my.csdn.net/my/mycsdn"
request = urllib2.Request(url,data)
response = urllib2.urlopen(request)
print response.read()
```

# 设置Headers
有些网站不会同意上面那样访问。

以知乎为例。

user-agent就是请求的身份（浏览器）。
```python:n
import urllib
import urllib2
url = 'http://www.server.com/login'
user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'
values = {'username' : 'cqc', 'password' : 'XXXX' }
headers = { 'User-Agent' : user_agent }
data = urllib.urlencode(values)
request = urllib2.Request(url, data, headers)
response = urllib2.urlopen(request)
page = response.read()
```
们还有对付”反盗链”的方式，对付防盗链，服务器会识别 headers 中的 referer 是不是它自己，如果
不是，有的服务器不会响应，所以我们还可以在 headers 中加入 referer
例如我们可以构建下面的headers
```
headers = { 'User-Agent' : 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)' ,
'Referer':'http://www.zhihu.com/articles' }
```
headers 的一些属性，下面的需要特别注意一下：
• User-Agent : 有些服务器或 Proxy 会通过该值来判断是否是浏览器发出的请求

• Content-Type : 在使用 REST 接口时，服务器会检查该值，用来确定 HTTP Body 中的内容该怎样解析。

• application/xml ： 在 XML RPC，如 RESTful/SOAP 调用时使用

• application/json ： 在 JSON RPC 调用时使用

• application/x-www-form-urlencoded ： 浏览器提交 Web 表单时使用

# Proxy的设置
urllib2 默认会使用环境变量 http_proxy 来设置 HTTP Proxy。假如一个网站它会检测某一段时间某个IP 的访问次数，如果访问次数过多，它会禁止你的访问。所以你可以设置一些代理服务器来帮助你做工作，每隔一段时间换一个代理，网站君都不知道是谁在捣鬼了。
```python:n
import urllib2
enable_proxy = True
proxy_handler = urllib2.ProxyHandler({"http" : 'http://some-proxy.com:8080'})
null_proxy_handler = urllib2.ProxyHandler({})
if enable_proxy:
opener = urllib2.build_opener(proxy_handler)
else:
opener = urllib2.build_opener(null_proxy_handler)
urllib2.install_opener(opener)
```

# Timeout
urlopen方法的第三个参数就是 timeout 的设置，可以设置等待多久超时，为了解决一些网站
实在响应过慢而造成的影响。
例如下面的代码,如果第二个参数 data 为空那么要特别指定是 timeout 是多少，写明形参，如果data已经传入，则不必声明。

# 使用HTTP的PUT和DELETE方法（不常用）
http 协议有六种请求方法，get,head,put,delete,post,options，我们有时候需要用到 PUT 方式或者 DELETE方式请求。
```
import urllib2
request = urllib2.Request(uri, data=data)
request.get_method = lambda: 'PUT' # or 'DELETE'
response = urllib2.urlopen(request)
```

# 使用DebugLog（不常用）
```python:n
import urllib2
httpHandler = urllib2.HTTPHandler(debuglevel=1)
httpsHandler = urllib2.HTTPSHandler(debuglevel=1)
opener = urllib2.build_opener(httpHandler, httpsHandler)
urllib2.install_opener(opener)
response = urllib2.urlopen('http://www.baidu.com')
```
# URLError异常处理
首先解释下 URLError 可能产生的原因：

• 网络无连接，即本机无法上网

• 连接不到特定的服务器

• 服务器不存在
```python:n
import urllib2
requset = urllib2.Request('http://www.xxxxx.com')
try:
urllib2.urlopen(requset)
except urllib2.URLError, e:
print e.reason
```
## HTTPError
发出请求后，服务器会response，若不能处理，会产生HTTPError，对应相应的状态码。
>100：继续 客户端应当继续发送请求。客户端应当继续发送请求的剩余部分，或者如果请求已经完成，忽略这个响应。
101： 转换协议 在发送完这个响应最后的空行后，服务器将会切换到在 Upgrade 消息头中定义的那些协议。只有在切换新的协议更有好处的时候才应该采取类似措施。
102：继续处理 由 WebDAV（RFC 2518）扩展的状态码，代表处理将被继续执行。
200：请求成功 处理方式：获得响应的内容，进行处理
201：请求完成，结果是创建了新资源。新创建资源的URI可在响应的实体中得到 处理方式：爬虫中不会遇到
202：请求被接受，但处理尚未完成 处理方式：阻塞等待
204：服务器端已经实现了请求，但是没有返回新的信 息。如果客户是用户代理，则无须为此更新自身的文档视图。 处理方式：丢弃
300：该状态码不被 HTTP/1.0 的应用程序直接使用， 只是作为 3XX 类型回应的默认解释。存在多个可用的被请求资源。 处理方式：若程序中能够处理，则进行进一步处理，如果程序中不能处理，则丢弃
301：请求到的资源都会分配一个永久的 URL，这样就可以在将来通过该 URL 来访问此资源 处理方式：重定向到分配的 URL
302：请求到的资源在一个不同的 URL 处临时保存 处理方式：重定向到临时的 URL
304：请求的资源未更新 处理方式：丢弃
400：非法请求 处理方式：丢弃
401：未授权 处理方式：丢弃
403：禁止 处理方式：丢弃
404：没有找到 处理方式：丢弃
500：服务器内部错误 服务器遇到了一个未曾预料的状况，导致了它无法完成对请求的处理。一般来说，这个问题都会在**服务器端**的源代码出现错误时出现。
501：服务器无法识别 服务器不支持当前请求所需要的某个功能。当服务器无法识别请求的方法，并且无法支持其对任何资源的请求。
502：错误网关 作为网关或者代理工作的服务器尝试执行请求时，从上游服务器接收到无效的响应。
503：服务出错 由于临时的**服务器**维护或者过载，服务器当前无法处理请求。这个状况是临时的，并且将在一段时间以后恢复。

父类的异常应当写到子类异常的后面。如果子类捕获不到，也还可以捕获父类的异常。所以：
```python:n
import urllib2
req = urllib2.Request('http://blog.csdn.net/cqcre')
try:
urllib2.urlopen(req)
except urllib2.HTTPError, e:
print e.code
except urllib2.URLError, e:
print e.reason
else:
print "OK"
```

最好加上属性判断，hasattr。以免属性输出报错。
```python:n
import urllib2
req = urllib2.Request('http://blog.csdn.net/cqcre')
try:
urllib2.urlopen(req)
except urllib2.URLError, e:
if hasattr(e,"code"):
print e.code
if hasattr(e,"reason"):
print e.reason
else:
print "OK"
```

# Cookie
## Opener
opener是urllib2.OpenerDirector的实例。

之前都是使用默认的opener，就是urlopen，一个特殊的opener。

如果需要用到cookie，这个opener达不到目的。所以需要创建更一般的opener。
## Cookielib
这个模块的主要作用是提供可存储cookie的对象。

可以利用这个模块的CookieJar类的对象来捕获cookie，并在后续连接请求时重新发送。

主要的对象有CookieJar---FileCookieJar---MozillaCookieJar&LWPCookieJar。（依次派生的关系）

首先利用CookieJar对象实现获取cookie的功能，存储到变量中。
```python:n
import urllib2
import cookielib
#声明一个CookieJar对象实例来保存cookie
cookie = cookielib.CookieJar()
#利用urllib2库的HTTPCookieProcessor对象来创建cookie处理器
handler=urllib2.HTTPCookieProcessor(cookie)
#通过handler来构建opener
opener = urllib2.build_opener(handler)
#此处的open方法同urllib2的urlopen方法，也可以传入request
response = opener.open('http://www.baidu.com')
for item in cookie:
    print 'Name = '+item.name
    print 'Value = '+item.value
```

然后将cookie保存到文件。用到FileCookieJar对象。这里使用它的子类MozillaCookieJar。
```python:n
import cookielib
import urllib2
#设置保存cookie的文件，同级目录下的cookie.txt
filename = 'cookie.txt'
#声明一个MozillaCookieJar对象实例来保存cookie，之后写入文件
cookie = cookielib.MozillaCookieJar(filename)
#利用urllib2库的HTTPCookieProcessor对象来创建cookie处理器
handler = urllib2.HTTPCookieProcessor(cookie)
#通过handler来构建opener
opener = urllib2.build_opener(handler)
#创建一个请求，原理同urllib2的urlopen
response = opener.open("http://www.baidu.com")
#保存cookie到文件
cookie.save(ignore_discard=True, ignore_expires=True)
```
>ignore_discard: save even cookies set to be discarded. ignore_expires: save even cookies that have expired The file is overwritten if it already exists

## 从文件中获取Cookie并访问
```
import cookielib
import urllib2
#创建MozillaCookieJar实例对象
cookie = cookielib.MozillaCookieJar()
#从文件中读取cookie内容到变量
cookie.load('cookie.txt',ignore_discard=True,ignore_expires=True)
#创建请求的request
req = urllib2.Request("http://www.baidu.com")
#利用urllib2的build_opener(urllib2.HTTPCookieProcessor(cookie))
response = opener.open(req)
print response.read()
```

## 利用cookie模拟网站登录
以教育系统为例：
```
import urllib
import urllib2
import cookielib

filename = 'cookie.txt'
#声明一个MozillaCookieJar对象实例保存cookie，之后写入文件
cookie = cookielib.MozillaCookieJar(filename)
opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cookie))
postdata = urllib.urlencode({
    'stuid':201200131012',
    'pwd':'23342321'
    })
#登录教务系统的URL
loginUrl = "http://jwxt.sdu.edu.cn:7890/pls/wwwbks/bks\_login2.login"
#模拟登录，并把cookie保存到变量
result = opener.open(loginUrl,postdata)
#保存cookie到cookie.txt中
cookie.save(ignore_discard=True,ignore\_expires=True)
#利用cookie请求访问另一个网址，此网址是成绩查询网址
gradeUrl = "http://jwxt.sdu.edu.cn:7890/pls/wwwbks/bkscjcx.curscopre"
#请求访问成绩查询网址
result = opener.open(gradeUrl)
print result.read()
```

# 正则表达式
## 贪婪模式与非贪婪模式
一般使用非贪婪模式，而默认是贪婪模式。

## 反斜杠问题
正则表达式里转义字符为反斜杠。
如果需要匹配文本中的字符'\'，那么正则表达式里面需要4个反斜杠，前两个和后两个分别用于在编程语言里转义成反斜杠，转换成反斜杠后再在正则表达式里转义成一个反斜杠。
haomafan~!

Python里的原生字符串解决了这个问题。`r"\"`。

## Python re模块
**返回pattern对象**：
re.compile(string[,flag])

**匹配**：
re.match(pattern,string[,flags])
re.search(pattern,string[,flags])
re.split(pattern,string[,maxsplit])
re.findall(pattern,string[,flags])
re.finditer(pattern,string[,flags])
re.sub(pattern,repl,string[,count])
re.subn(pattern,repl,string[,count])


![](~/12-33-10.jpg)

**match对象的属性和方法**：
属性：
![](~/12-37-33.jpg)
方法：
![](~/12-38-27.jpg)

```python:n
#match实例

import re

m = re.match(r'(\w+)(\w+)','hello world!')

print "m.string:",m.string
print "m.re:",m.re
print "m.pos:",m.pos
print "m.endpos:",m.endpos
print "m.lastindex:",m.lastgroup
print "m.group():",m.group()
print "m.group(1,2):",m.group(1,2)
print "m.groups():",m.groups()
print "m.groupdict():",m.groupdict()
print "m.start(2):",m.start(2)
print "m.end(2):",m.end(2)
print "m.span(2):",m.span(2)
print r"m.expand(r'\g \g'):",m.expand(r'\2 \1')
```

### re.search()
和match类似。但是match只检测开始位置匹配否，search扫描整个string。
### re.split()
maxsplit用于指定最大分割次数，默认全部分割。
### re.findall()
### re.finditer()
搜索string，返回一个顺序访问每一个匹配结果（match对象）的迭代器。
```python:n
import re
pattern = re.compile(r'\d+')
for m in re.findite(pattern,'one1two2three3four4'):
    print m.group()

#ouput#
#1 2 3 4#
```
### re.sub()
替换匹配的字符串，并返回替换后的字符串。
repl是一个字符串时，可以使用\id或\g、\g引用分组，不能使用编号0。
repl是一个方法时，这个方法只接受一个match对象作为参数，并返回一个字符串用于替换（返回的字符串不能再引用分组）。

```python:n
import re
pattern = re.compile(r'(\w+)(\w+)')
s = 'i say, hello world!'
print re.sub(pattern,r'\2 \1',s)
def func(m):
    return m.group(1).title() + ' ' + m.group(2).title()
print re.sub(pattern,func,s)

#output#
# say i, world hello! #
# I say, Hello World! #
```
### re.subn()
返回(替换后的字符串,替换次数)。如下：
```python:n
import re
pattern  = re.compile(r'(\w+)(\w+)')
s = 'i say, hello world!'
print re.subn(pattern,r'\2 \1',s)
def func(m):
    return m.group(1).title() + ' ' + m.group(2).title()
print re.subn(pattern,func,s)

#output#
# ('say i, world hello!',2) #
# ('I Say, Hello World!',2) #
```

## Python re模块的另一种使用方式
pattern.match(),pattern.search()，这样调用就不用将pattern作为第一个参数传入。

# Beautiful Soup的用法
四大对象：
* Tag : HTML标签
* NavigableString：标签内部的文字
* BeautifulSoup：一个文档的全部内容
* Comment：一个特殊类型的NavigableString对象

# 爬取糗事百科段子
目标：
1. 抓取糗事百科热门段子（不需要登录）
2. 过滤带有图片的段子
3. 每按一次回车，显示一个段子


## 确定URL并抓取页面代码
```
# -*-coding:utf-8 -*-
import urllib
import urllib2

page = 1
url = 'http://www.qiushibaike.com/hot/page/' + str(page)

user_agent = 'Mozilla/4.0(compatible;MSIE 5.5;Windows NT)'
headers = {'User-Agent':user_agent}

try:
    request = urllib2.Request(url,headers=headers)
    response = urllib2.urlopen(request)
    print response.read()
except urllib2.URLError,e:
    if hasattr(e,"code"):
        print e.code
    if hasattr(e,"reason"):
        print e.reason
```

## 抓取某一页的所有段子
```python:n
# -*-coding:utf-8 -*-
import urllib
import urllib2
import re
import thread
import time


# page = 12
# url = 'http://www.qiushibaike.com/hot/page/' + str(page)
#
# user_agent = 'Mozilla/4.0(compatible;MSIE 5.5;Windows NT)'
# headers = {'User-Agent':user_agent}
#
#
# try:
#     request = urllib2.Request(url,headers=headers)
#     response = urllib2.urlopen(request)
#     #print response.read()
#
#     content = response.read().decode('utf-8')
#     pattern = re.compile('<div.?class="author.?>.?<a.?.?<a.?>(.?).?<div.?class'+'="content".?title="(.?)">(.?)(.*?)<div class="stats.?class="number">(.?)',re.S)
#     # re.S代表在匹配时为点任意匹配模式，点.也可以代表换行符
#     items = re.findall(pattern,content)
#     for item in items:
#         haveImg = re.search("img",item[3])
#         if not haveImg:
#             print item[0],item[1],item[2],item[4]
# except urllib2.URLError,e:
#     if hasattr(e,"code"):
#         print e.code
#     if hasattr(e,"reason"):
#         print e.reason


class qsbk:
    def __init__(self):
        self.pageIndex = 12
        self.user_agent = 'Mozilla/4.0(compatible;MSIE 5.5;Windows NT)'
        self.headers = {'User-Agent':self.user_agent}
        self.stories = []   #段子
        self.enable = False #是否继续运行
    def getPage(self,pageIndex):
        try:
            url = 'http://www.qiushibaike.com/hot/page/' + str(pageIndex)
            request = urllib2.Request(url,headers=self.headers)
            response = urllib2.urlopen(request)
            pageCode = response.read().decode('utf-8')
            return pageCode
        except urllib2.URLError,e:
            if hasattr(e,"code"):
                print e.code
            if hasattr(e,"reason"):
                print u'连接糗事百科失败，错误原因：',e.reason
                return None
    def getPageItems(self,pageIndex):
        pageCode = self.getPage(pageIndex)
        if not pageCode:
            print "页面加载失败……"
            return None
        pattern = re.compile('<div.?class="author.?>.?<a.?.?<a.?>(.?).?<div.?class'+'="content".?title="(.?)">(.?)(.*?)<div class="stats.?class="number">(.?)',re.S)
        items = re.findall(pattern,pageCode)
        pageStories = []
        for item in items:
            #是否含有图片
            haveImg = re.search("img",item[3])
            if not haveImg:
                #item[0]是段子发布者，item[1]是发布时间,item[2]是内容,item[4]是点赞数
                pageStories.append([item[0].strip(),item[1].strip(),item[2].strip(),item[4].strip()])
        return pageStories

    def loadPage(self):
        #如果当前未看的页数少于2页，则加载新一页
        if self.enable == True:
            if len(self.stories)<2:
                pageStories = self.getPageItems(self.pageIndex)
                #将该页的段子存放到全局list中
                if pageStories:
                    self.stories.append(pageStories)
                    self.pageIndex += 1

    def getOneStory(self,pageStories,page):
        #遍历一页的段子
        for story in pageStories:
            input = raw_input()
            self.loadPage()
            if input == 'Q':
                self.enable = False
                return
            print u'第%d页\t发布人：%s\t发布时间：%s\n%s\n赞：%s\n' % (page,story[0],story[1],story[2],story[3])

    def start(self):
        print u"正在读取糗事百科，按回车查看新段子，按Q退出"
        self.enable = True
        self.loadPage()
        nowPage = 0
        while self.enable:
            if len(self.stories)>0:
                #从全局list中获取一页的段子
                pageStories = self.stories[0]
                nowPage += 1
                #将全局list中第一个元素删除，因为已经取出
                del self.stories[0]
                #输出该页的段子
                self.getOneStory(pageStories,nowPage)

spider = qsbk()
spider.start()
```

# 爬取百度贴吧帖子
目标：
1. 对百度贴吧的任意帖子进行抓取；
2. 指定是否只抓取楼主发帖内容；
3. 将抓取到的内容分析并保存到文件。

## URL格式的确定
<http://tieba.baidu.com/p/3138733512?see_lz=1&pn=1>

![](~/23-25-43.jpg)

将URL划分为基础部分和参数部分。
## 页面的抓取
```python:n
#-*- coding:utf-8 -*-
import urllib
import urllib2
import re

#百度贴吧爬虫类
class BDTB:
    #初始化，传入基础地址，是否只看楼主的参数
    def __init__(self,baseUrl,seeLz):
        self.baseURL = baseUrl
        self.seeLz = '?see_lz='+str(seeLz)

    #传入页码，获取该页帖子的代码
    def getPage(self,pageNum):
        try:
            url = self.baseURL + self.seeLz + '&pn=' + str(pageNum)
            request = urllib2.Request(url)
            response = urllib2.urlopen(request)
            print response.read()
            return response
        except urllib2.URLError,e:
            if hasattr(e,"reason"):
                print u"连接百度贴吧失败,错误原因",e.reason

                return None

baseURL = 'http://tieba.baidu.com/p/3138733512'
bdtb = BDTB(baseURL,1)
bdtb.getPage(1)
```
## 提取相关信息
**提取帖子标题**

正则表达式：
`<h1 class="core_title_txt.*?>(.*?)</h1>`


```python:n
def getTitle(self):
    page = self.getPage(1)
    pattern = re.compile('<h1 class="core_title_txt.*?>(.*?)</h1>',re.S)
    result = re.search(pattern,page)
    if result:
        #print result.group(1) #测试输出
        return result.group(1).strip()
    else:
        return None
```

**提取帖子页数**

```python:n
def getPageNum(self):
    page = self.getPage(1)
    pattern = re.compile('<li class="L_reply_num.*?</span>.*?<span.*?>(.*?)</span>',re.S)
    result = re.search(pattern,page)
    if result:
        #print result.group(1)
        return result.group(1).strip()
    else:
        return None
```

**提取正文内容**


```python:n
#-*- coding:utf-8 -*-
import urllib
import urllib2
import re

class TOOL:
    #去除img标签，7位长空格
    removeImg = re.compile('<img.*?>| {7}|')
    #删除超链接标签
    removeAddr = re.compile('<a.*?>|</a>')
    #把换行的标签换为\n
    replaceLine = re.compile('<tr>|<div>|</div>|</p>')
    #将制表符<td>换为\t
    replaceTD = re.compile('<td>')
    #把段落开头换为\n加空两格
    replacePara = re.compile('<p.*?>')
    #将换行符或双换行符替换为\n
    replaceBR = re.compile('<br><br>|<br>')
    #将其余标签剔除
    removeExtraTag = re.compile('<.*?>')
    def replace(self,x):
        x = re.sub(self.removeImg,"",x)
        x = re.sub(self.removeAddr,"",x)
        x = re.sub(self.replaceLine,"\n",x)
        x = re.sub(self.replaceTD,"\t",x)
        x = re.sub(self.replacePara,"\n  ",x)
        x = re.sub(self.replaceBR,"\n",x)
        x = re.sub(self.removeExtraTag,"",x)
        #strip()将前后多余内容剔除
        return x.strip()

#百度贴吧爬虫类
class BDTB:
    #初始化，传入基础地址，是否只看楼主的参数
    def __init__(self,baseUrl,seeLz):
        self.baseURL = baseUrl
        self.seeLz = '?see_lz='+str(seeLz)

    #传入页码，获取该页帖子的代码
    def getPage(self,pageNum):
        try:
            url = self.baseURL + self.seeLz + '&pn=' + str(pageNum)
            request = urllib2.Request(url)
            response = urllib2.urlopen(request)
            #print response.read()
            return response.read().decode('utf-8')
        except urllib2.URLError,e:
            if hasattr(e,"reason"):
                print u"连接百度贴吧失败,错误原因",e.reason

                return None
    def getContent(self,page):
        pattern = re.compile('<div id="post_content_.*?>(.*?)</div>',re.S)
        items = re.findall(pattern,page)
        for item in items:
            print item



baseURL = 'http://tieba.baidu.com/p/3138733512'
bdtb = BDTB(baseURL,1)
bdtb.getContent(bdtb.getPage(1))
```

# 计算大学本学期绩点

![](~/23-36-35.jpg)

# 抓取淘宝MM照片
目标：
1. 抓取淘宝MM的姓名、头像、年龄；
2. 抓取每个MM的资料简介以及写真图片
3. 把每个MM的写真图片按照文件夹保存到本地
4. 熟悉文件保存的过程
```python:n
# URL = https://mm.taobao.com/json/request_top_list.htm?page=1
# 问号前为基地址

import urllib
import urllib2
import re

class TOOL:
    #去除img标签，7位长空格
    removeImg = re.compile('<img.*?>| {7}|')
    #删除超链接标签
    removeAddr = re.compile('<a.*?>|</a>')
    #把换行的标签换为\n
    replaceLine = re.compile('<tr>|<div>|</div>|</p>')
    #将制表符<td>换为\t
    replaceTD = re.compile('<td>')
    #把段落开头换为\n加空两格
    replacePara = re.compile('<p.*?>')
    #将换行符或双换行符替换为\n
    replaceBR = re.compile('<br><br>|<br>')
    #将其余标签剔除
    removeExtraTag = re.compile('<.*?>')
    def replace(self,x):
        x = re.sub(self.removeImg,"",x)
        x = re.sub(self.removeAddr,"",x)
        x = re.sub(self.replaceLine,"\n",x)
        x = re.sub(self.replaceTD,"\t",x)
        x = re.sub(self.replacePara,"\n  ",x)
        x = re.sub(self.replaceBR,"\n",x)
        x = re.sub(self.removeExtraTag,"",x)
        #strip()将前后多余内容剔除
        return x.strip()


class Spider:
    def __init__(self):
        self.siteURL = 'https://mm.taobao.com/json/request_top_list.htm'
        self.tool = TOOL()

    def getPage(self,pageIndex):
        url = self.siteURL + '?page=' + str(pageIndex)
        #print url
        request = urllib2.Request(url)
        response = urllib2.urlopen(request)
        return response.read().decode('gbk')

    def getContents(self,pageIndex):
        page = self.getPage(pageIndex)
        pattern = re.compile('<div class="list-item".*?pic-word.*?<a href="(.*?)".*?<img src="(.*?)".*?<a class="lady-name.*?')
        items = re.findall(pattern,page)
        contents = []
        for item in items:
            contents.append([item[0],item[1],item[2],item[3],item[4]])
            #print item[0],item[1],item[2],item[3],item[4]
        return contents

    def getAllImg(self,page):
        pattern = re.compile('<div class="mm-aixiu-content".*?>(.*?)<!--',re.S)
        #个人信息页面所有代码
        content = re.search(pattern,page)
        #从代码中提取图片
        patternImg = re.compile('<img.*?src="(.*?)"',re.S)
        images = re.findall(patternImg,content.group(1))
        return images
    def saveImgs(self,images,name):
        number = 1
        print u"ixan",name,u"共有",len(images),u"张照片"
        for imageURL in images:
            splitPath = imageURL.split('.')
            fTail = splitPath.pop()
            if len(fTail)>3:
                fTail = 'jpg'
            fileName = name + "/" + str(number) + "." + fTail
            self.saveImg(imageURL,fileName)
            number += 1
    #保存头像
    def saveIcon(self,iconURL,name):
        splitPath = iconURL.split('.')
        fTail = splitPath.pop()
        fileName = name + '/icon.' + fTail
        self.saveImg(iconURL,fileName)


    def saveImg(self,imageURL,fileName):
        u = urllib.urlopen(imageURL)
        data = u.read()
        f = open(fileName,'wb')
        f.write(data)
        print u"正在悄悄保存她的一张图片为",fileName
        f.close()

    #保存个人简介
    def saveBrief(self,content,name):
        fileName = name + "/" + name + ".txt"
        f = open(fileName,"w+")
        print u"正在偷偷保存她的个人信息为",fileName
        f.write(content.encode('utf-8'))

    def mkdir(self,path):
        path = path.strip()
        #判断路径是否存在
        isExists = os.path.exists(path)
        #判断结果
        if not isExists:
            print u"偷偷新建了名字叫做",path,u"的文件夹"
            os.makedirs(path)
            return True
        else:
            print u"名为",path,"的文件夹已经创建成功"
            return False
    def savePageInfo(self,pageIndex):
        #获取第一页淘宝mm列表
        contents = self.getContents(pageIndex)
        for item in contents:
            #item[0]个人详情URL
            #item[1]头像URL
            #item[2]姓名
            #item[3]年龄
            #item[4]居住地
            print u"发现一位模特，名字叫",item[2],u"芳龄",item[3],u"她在",item[4]
            print u"正在偷偷保存",item[2],u"的信息"
            print u"又意外发现她的个人地址是",item[0]
            #个人详情页面URL
            detailURL = item[0]
            detailPage = self.getDetailPage(detailURL)
            brief = self.getBrief(detailPage)
            images = self.getAllImg(detailPage)
            self.mkdir(item[2])
            self.saveBrief(brief,item[2])
            self.saveIcon(item[1],item[2])
            self.saveImgs(images,item[2])

    #传入起止页码，获取mm图片
    def savePagesInfo(self,start,end):
        for i in range(start,end+1)
            print u"正在偷偷寻找第",i,u"个地方，看看mm们在不在"
            self.savePageInfo(i)

spider = Spider()
#spider.getContents(1)
spider.savePagesInfo(2,10)
```
