## HTTP服务
使用缓存这种架构要求应用对数据的一致性要求不是很高；比如像下订单这种要落地的数据不适合用 Redis 存储，但是订单的读取可以使用缓存。
![](http://opkk27k9n.bkt.clouddn.com/17-6-28/52489916.jpg)

以获取京东商品页广告词为例:
![](http://opkk27k9n.bkt.clouddn.com/17-6-28/6276582.jpg)
在设计时就要考虑：
1. 数据量，数据更新是否频繁且更新量是否很大；
2. 是K-V还是关系，是否需要批量获取，是否需要按照规则查询。\

而对于本例，广告词更新量不会很大，每分钟可能在几万左右；而且是 K-V 的，其实适合使用关系存储；因为广告词是商家维护，因此后台查询需要知道这些商品是哪个商家的；而对于前台是不关心商家的，是 KV 存储，所以前台显示的可以放进如 Redis 中。 即存在两种设计：
1. 所有数据存储到 Mysql，然后热点数据加载到 Redis；
2. 关系存储到 Mysql，而数据存储到如SSDB这种持久化KV存储中。

基本数据结构：商品 ID、广告词、所属商家、开始时间、结束时间、是否有效。

### 后台逻辑
1. 商家登录后台；
2. 按照商家分页查询商家数据，此处要按照商品关键词或商品类目查询的话，需要走商品系统的搜索子系统，如通过 Solr或elasticsearch 实现搜索子系统；
3. 进行广告词的增删改查；
4. 增删改时可以直接更新 Redis 缓存或者只删除 Redis 缓存（第一次前台查询时写入缓存）；

### 前台逻辑
1. 首先 Nginx 通过 Lua 查询 Redis 缓存；
2. 查询不到的话回源到 Tomcat，Tomcat 读取数据库查询到数据，然后把最新的数据异步写入 Redis（一般
设置过期时间，如5分钟）；此处设计时要考虑假设 Tomcat 读取 Mysql 的极限值是多少，然后设计降级开
关，如假设每秒回源达到 100，则直接不查询 Mysql 而返回空的广告词来防止 Tomcat 应用雪崩。


Tomcat后台项目部署目录结构:

```
/usr/chapter6
    redis_6660.conf
    redis_6661.conf
    nginx_chapter6.conf
    nutcracker.yml
    nutcracker.init
    webapp
WEB-INF
    lib
    classes
    web.xml
```

### Redis+Twemproxy 配置
根据实际情况来决定 Redis 大小，此处我们已两个 Redis 实例（6660、6661），在Twemproxy 上通过一致性 Hash 做分片逻辑。
```conf
#分别为6660 6661
port 6660
#进程ID 分别改为redis_6660.pid redis_6661.pid
pidfile "/var/run/redis_6660.pid"
#设置内存大小，根据实际情况设置，此处测试仅设置20mb
maxmemory 20mb
#内存不足时，按照过期时间进行LRU删除
maxmemory-policy volatile-lru
#Redis的过期算法不是精确的而是通过采样来算的，默认采样为3个，此处我们改成10
maxmemory-samples 10
#不进行RDB持久化
save “”
#不进行AOF持久化
appendonly no
```

Twemproxy配置nutcracker.yml
```yml
server1:
listen: 127.0.0.1:1111
hash: fnv1a_64
distribution: ketama
redis: true
timeout: 1000
servers:
    - 127.0.0.1:6660:1 server1
    - 127.0.0.1:6661:1 server2
```

启动：
```
nohup /usr/servers/redis-2.8.19/src/redis-server /usr/chapter6/redis_6660.conf &
nohup /usr/servers/redis-2.8.19/src/redis-server /usr/chapter6/redis_6661.conf &
/usr/chapter6/nutcracker.init start
ps -aux | grep -e redis -e nutcracker
```

# Mysql + Atlas配置
Atlas 类似于 Twemproxy，是 Qihoo 360 基于 Mysql Proxy 开发的一个 Mysql 中间件，据称每天承载读写请求数达几十亿，可以实现分表、读写分离、数据库连接池等功能，缺点是没有实现跨库分表（分库）功能，需要在客户端使用分库逻辑。另一个选择是使用如阿里的 TDDL，它是在客户端完成之前说的功能。到底选择是在客户端还是在中间件根据实际情况选择。此处我们不做 Mysql 的主从复制（读写分离），只做分库分表实现。

建两个表：
```
CREATE DATABASE chapter6 DEFAULT CHARACTER SET utf8;
use chapter6;
CREATE TABLE chapter6.ad_0(
sku_id BIGINT,
content VARCHAR(4000)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
CREATE TABLE chapter6.ad_1
sku_id BIGINT,
content VARCHAR(4000)
) ENGINE=InnoDB DEFAULT CHARSET=utf8
```

### Atlas配置
```conf
<!--[mysql-proxy]-->
#Atlas代理的主库，多个之间逗号分隔
proxy-backend-addresses = 127.0.0.1:3306
#Atlas代理的从库，多个之间逗号分隔，格式ip:port@weight，权重默认1
#proxy-read-only-backend-addresses = 127.0.0.1:3306,127.0.0.1:3306
#用户名/密码，密码使用/usr/servers/Atlas-2.2.1/script/encrypt 123456加密
pwds = root:/iZxz+0GRoA=
#后端进程运行
daemon = true
#开启monitor进程，当worker进程挂了自动重启
keepalive = true
#工作线程数，对Atlas的性能有很大影响，可根据情况适当设置
event-threads = 64
#日志级别
log-level = message
#日志存放的路径
log-path = /usr/chapter6/
#实例名称，用于同一台机器上多个Atlas实例间的区分
instance = test
#监听的ip和port
proxy-address = 0.0.0.0:1112
#监听的管理接口的ip和port
admin-address = 0.0.0.0:1113
#管理接口的用户名
admin-username = admin
#管理接口的密码
admin-password = 123456
#分表逻辑
tables = chapter6.ad.sku_id.2
#默认字符集
charset = utf8
```
因为本例没有做读写分离，所以读库 proxy-read-only-backend-addresses 没有配置。分表逻辑即：数据库名.表名.分表键.表的个数，分表的表名格式是 table_N，N 从 0 开始。

Atlas启动/重启/停止
```sh
/usr/local/mysql-proxy/bin/mysql-proxyd chapter6 start
/usr/local/mysql-proxy/bin/mysql-proxyd chapter6 restart
/usr/local/mysql-proxy/bin/mysql-proxyd chapter6 stop
```


- Mysql 主从复制 http://369369.blog.51cto.com/319630/790921/ 
- Mysql中间件介绍 http://www.guokr.com/blog/475765/ 
- Atlas使用 http://www.0550go.com/database/mysql/mysql-atlas.html 
- Atlas文档 (https://github.com/Qihoo360/Atlas/blob/master/README_ZH.md href="https://github.com/Qihoo360/Atlas/blob/master/README_ZH.md")

在一个服务器上能启动两个 tomcat 实例，这样的好处是我们可以做本机的 Tomcat 负载均衡，假设一个 tomcat 重启时另一个是可以工作的，从而不至于不给用户返回响应。
![](http://opkk27k9n.bkt.clouddn.com/17-6-28/59655030.jpg)

upstream 配置：http://nginx.org/cn/docs/http/ngx_http_upstream_module.html。

- server：指定上游到的服务器， 
- weight：权重，权重可以认为负载均衡的比例； 
- fail_timeout+max_fails：在指定时间内失败多少次认为服务器不可用，通过proxy_next_upstream来判断是否失败。
- check：ngx_http_upstream_check_module模块，上游服务器的健康检查，
- interval：发送心跳包的时间间隔， 
- rise：连续成功rise次数则认为服务器up，
- fall：连续失败fall次则认为服务器down，
- timeout：上游服务器请求超时时间，
- type：心跳检测类型（比如此处使用 tcp ）更多配置请参考 [https://github.com/yaoweibin/nginx_upstream_check_module](https://github.com/yaoweibin/ nginx_upstream_check_module) 和http://tengine.taobao.org/document_cn/http_upstream_check_cn.html。
- keepalive：用来支持 upstream server http keepalive 特性(需要上游服务器支持，比如 tomcat )。默认的负载均衡算法是 round-robin，还可以根据 ip、url 等做 hash 来做负载均衡。更多资料请参考官方文档。tomcat keepalive 配置： http://tomcat.apache.org/tomcat-7.0-doc/config/http.html。 maxKeepAliveRequests：默认100； 
- keepAliveTimeout：默认等于 connectionTimeout，默认 60 秒；location proxy 配置：`http://nginx.org/cn/docs/http/ngx_http_proxy_module.html`。 - - - - rewrite：将当前请求的 url 重写，如我们请求时是 /backend/ad，则重写后是 /ad。 
- proxy_pass：将整个请求转发到上游服务器。
- proxy_next_upstream：什么情况认为当前 upstream server 失败，需要 next upstream，默认是连接失败/超时，负载均衡参数。 
- proxy_pass_request_headers：之前已经介绍过了，两个原因：1、假设上游服务器不需要请求头则没必要传输请求头；2、ngx.location.capture时防止 gzip 乱码（也可以使用more_clear_input_headers 配置）。 
- keepalive：keepalive_timeout：keepalive 超时设置，keepalive_requests：长连接数量。此处的 keepalive（别人访问该 location 时的长连接）和 upstream keepalive（nginx 与上游服务器的长连接）是不一样的；此处注意，如果您的服务是面向客户的，而且是单个动态内容就没必要使用长连接了。


核心代码：
```lua
local redis = require("resty.redis")
local cjson = require("cjson")
local cjson_encode = cjson.encode
local ngx_log = ngx.log
local ngx_ERR = ngx.ERR
local ngx_exit = ngx.exit
local ngx_print = ngx.print
local ngx_re_match = ngx.re.match
local ngx_var = ngx.var
local function close_redis(red)
if not red then
return
end
--释放连接(连接池实现)
local pool_max_idle_time = 10000 --毫秒
local pool_size = 100 --连接池大小
local ok, err = red:set_keepalive(pool_max_idle_time, pool_size)
if not ok then
ngx_log(ngx_ERR, "set redis keepalive error : ", err)
end
end
local function read_redis(id)
local red = redis:new()
red:set_timeout(1000)
local ip = "127.0.0.1"
local port = 1111
local ok, err = red:connect(ip, port)
if not ok then
ngx_log(ngx_ERR, "connect to redis error : ", err)
return close_redis(red)
end
local resp, err = red:get(id)
if not resp then
ngx_log(ngx_ERR, "get redis content error : ", err)
return close_redis(red)
end
--得到的数据为空处理
if resp == ngx.null then
resp = nil
end
close_redis(red)
return resp
end
local function read_http(id)
local resp = ngx.location.capture("/backend/ad", {
method = ngx.HTTP_GET,
args = {id = id}
})
if not resp then
ngx_log(ngx_ERR, "request error :", err)
return
end
if resp.status ~= 200 then
ngx_log(ngx_ERR, "request error, status :", resp.status)
return
end
return resp.body
end
--获取id
local id = ngx_var.id
--从redis获取
local content = read_redis(id)
--如果redis没有，回源到tomcat
if not content then
ngx_log(ngx_ERR, "redis not found content, back to http, id : ", id)
content = read_http(id)
end
--如果还没有返回404
if not content then
ngx_log(ngx_ERR, "http not found content, id : ", id)
return ngx_exit(404)
end
--输出内容
ngx.print("show_ad(")
ngx_print(cjson_encode({content = content}))
ngx.print(")")
```

### nginx proxy cache
为了防止恶意刷页面/热点页面访问频繁，我们可以使用 nginx proxy_cache 做页面缓存，当然更好的选择是使用 CDN 技术，如通过 Apache Traffic Server、Squid、Varnish。

nginx.conf
```conf
proxy_buffering on;
proxy_buffer_size 8k;
proxy_buffers 256 8k;
proxy_busy_buffers_size 64k;  
proxy_temp_file_write_size 64k;
proxy_temp_path /usr/servers/nginx/proxy_temp;
k#设置Web缓存区名称为cache_one，内存缓存空间大小为200MB，1分钟没有被访问的内容自动清除，硬盘缓存空间大小为30GB。
proxy_cache_path /usr/servers/nginx/proxy_cache levels=1:2 keys_zone=cache_item:200m inactive=1m max_size=30g
```